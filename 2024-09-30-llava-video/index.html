<!DOCTYPE html>
<html>

    

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title> LLaVA-Video: Video Instruction Tuning with Synthetic Data</title>

  <link rel="icon" type="image/x-icon" href="/blog/assets/images/logos/favicon.ico">


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
      .center-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100%;
            margin-top: -20px;
        }
    .node {
      fill: #f8f1e4;
      stroke: #000;
      stroke-width: 1;
      rx: 10;
      ry: 10;
    }
    .node text {
      font-size: 14px;
      text-anchor: middle;
    }
    .link {
      fill: none;
      stroke: #000;
      stroke-width: 2;
    }
    .badge {
      font-size: 12px;
    }
  </style>

  <style>
    /* Basic styling for the bar */
    .navbar {
        background-color: #333;
        overflow: hidden;
    }
    
    .navbar a {
        float: left;
        display: block;
        color: #f2f2f2;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
    }
    
    .navbar a:hover {
        background-color: #ddd;
        color: black;
    }
  </style>

</head>
<body>


    <!-- Navigation Bar -->
  <div class="navbar">
      <a href="https://llava-vl.github.io/blog">Blog Series</a>
  </div>  

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true">LLaVA Blog</span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold"  style="display: inline-block">
            <span style="vertical-align: middle">Video Instruction Tuning with Synthetic Data</span>
            </h1>
          <br>
          <!-- <h2 class="subtitle is-3 publication-subtitle" style="display: inline-block">
            <span style="vertical-align: middle">Video Instruction Tuning with Synthetic Data</span>
          </h2> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a><sup style="color:#9400D3;">&dagger;</sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/kiming-ng">Jinming Wu</a><sup style="color:#008000;">&ddagger;</sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=q8ZrKVIAAAAJ&hl=zh-CN">Wei Li</a><sup style="color:#bf906f;">&natural;</sup>,</span>
            
            <span class="author-block">
              <a href="https://brianboli.com/">Bo Li</a><sup style="color:#9400D3;">&dagger;</sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XwY9LXoAAAAJ&hl=zh-CN">Zejun Ma</a><sup style="color:#bf906f;">&natural;</sup></span><br>
            
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup style="color:#9400D3;">&dagger;</sup><sup style="color:#6fbf73;">*</sup>,
            </span>
            <span class="author-block" style="margin-bottom: 10px;">
              <a href="https://chunyuan.li/">Chunyuan Li</a><sup style="color:#bf906f;">&natural;</sup><sup style="color:#6fbf73;">*</sup>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#bf906f;">&natural;</sup>ByteDance,</span>
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#9400D3;">&dagger;</sup>NTU S-Lab,</span>
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#008000;">&ddagger;</sup>BUPT</span><br>
            <span class="author-block"><sup style="color:#6fbf73;">â™¡</sup> Work collaborated with ByteDance</span>
            <span class="author-block"><sup style="color:#6fbf73;">*</sup> Co-senior authors</span><br>
            <!-- <span class="paper-block"><b style="color:#f41c1c">ICLR 2024 Oral</b> (85 in 7304, 1.2%)</span> -->
          </div>
          
          
        

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href="https://arxiv.org/pdf/2408.03326"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸŽ¬</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Dataset: LLaVA-Video-178K</span>
                </a>
              </span> 
              <span class="link-block">
                <a href="http://arxiv.org/abs/2410.02713"
                   class="external-link button is-normal is-rounded is-dark">
                <!-- <a href="https://lupantech.github.io/papers/arxiv23_mathvista.pdf"
                   class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv Paper</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/papers/2410.02713"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Huggingface Page</span>
                </a>
              </span>               
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/LLaVA-VL/LLaVA-NeXT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Training Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/lmms-lab/llava-video-661e86f5e8dabc3ff793c944"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Checkpoints</span>
                </a>
              </span> 
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/Tonic/Llava-Video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">ðŸŽ¨</p>
                  </span>
                  <span>Tonic's Demo</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p>
            The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we consider an alternative approach, creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this proposed dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.  

              <br><br>
              
              Click on the sections below to learn more about this project  :

              <ol class="text">
                  <li><strong><a href="#LLaVA-178K">&sect;Video Instruction-Following Data Synthesis</a></strong></li>
                  <li><strong><a href="#Video Representation">&sect;Video Representation</a></strong></li>
                  <li><strong><a href="#Benchmark Performance">&sect;Benchmark Performance</a></strong></li>
                  <li><strong><a href="#Interactive Demos">&sect;Interactive Demos</a></strong></li>
              </ol>

            </p>

        </div>


      </div>
    </div>
  </div>
</section>


<section class="section" id="LLaVA-178K">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video Instruction-Following Data Synthesis</h2>
        <div class="content has-text-justified">
            <p>
              A high-quality dataset for video instruction-tuning is crucial for developing effective video-language models. We identify a key factor in building such datasets: ensuring richness and diversity in both video content and its language annotations. We perform comprehensive survey on the existing video benchmarks, covering across various public video captioning and question-answering datasets, then identify ten unique video sources that contribute to over 40 video-language benchmarks. From each source, we select videos that exhibit significant temporal dynamics. To maintain diversity in the annotations, we establish a pipeline capable of generating detailed captions for videos of any length. Additionally, we define 16 types of questions that guide GPT-4o in creating question-answer pairs to assess the perceptual and reasoning skills of the video-language models.  
            </p>


            <h3 class="title is-4">Video Sources</h3>
            We noticed that although different video-language datasets focus on various video understanding tasks , most are sourced from ten main video sources, which offer a wide range of video data from different websites, viewpoints, and domains. The relationship between these ten selected video datasets and others is shown in figure below. We select the dynamic video from these source, we detail the video selection logic in the paper.
            <br><br>
            <d-figure id="fig-comparison" >
              <figure>
                  <img data-zoomable="" draggable="false" src="static/images/video_source_page-0001.jpg" alt="benchmark category">
                  <figcaption>
                      <strong>Figure 1:</strong> The relationship between 10 video sources we have utilized and other existing video-language datasets.
                  </figcaption>
              </figure>
            </d-figure>
            
            <h3 class="title is-4">Automated Generation for Video Detail Description</h3>
            For selected videos, we use GPT-4o to systematically describe their content. We start by sampling video frames at one frame per second (fps). However, due to the input size constraints of GPT-4o, we cannot use all sampled frames. Instead, we describe the videos sequentially, as shown in figure below. We create descriptions at three distinct levels, detailed below.
            <br><br>
            <d-figure id="fig-comparison" >
              <figure>
                  <img data-zoomable="" draggable="false" src="static/images/llava_video_data_creation_pages-to-jpg-0001.jpg" alt="benchmark category">
                  <figcaption>
                      <strong>Figure 2:</strong> A three-level creation pipeline is considered, with each level developed via a recurrent approach. 
                      Note that t is the index of time internal at its own level, and T is the last time internal index. 
                      (a) To generate the caption for time internal t at level-1, we condition on the current frames in this internal, the caption for time internal t-1, and the most recent description summary at level-2 if applicable. 
                      (b) To generate caption for time internal t at level-2, we condtion on the previous caption at level-2, and captions from three most recent time internals at level-1.
                      (c) To generate the overall caption at the last time internal t at level-3, we condtion on the the most recent caption at level-2 and the current caption from level-1.
                  </figcaption>
              </figure>
            </d-figure>      

            <h3 class="title is-4">Automated Generation for Video Question Answering</h3>
            In addition to detailed video descriptions, our dataset includes a variety of question-answer pairs designed for complex interactions. This setup improves the video understanding model's ability to handle real-life queries. We refer to public video question-answering benchmarks to organize these questions into 16 specific categories, as shown in Figure 3. Given a detailed video description, we use GPT-4o to generate at most one question-answer pair for each type of question. Please refer to the paper for more details of the question types and the generation process.
            <d-figure id="fig-comparison" >
              <figure>
                  <img data-zoomable="" draggable="false" src="static/images/question_type_page-0001.jpg" alt="benchmark category">
                  <figcaption>
                      <strong>Figure 3:</strong> Question types for video question answering in data creation. For each type, we provide its name and an example question.
                  </figcaption>
              </figure>
            </d-figure>     
            
            <h3 class="title is-4">Dataset Statistics</h3>
            We carefully select from our collected data sources to form a balanced and comprehensive collection, resulting in a total of 178K videos and 1.3M instruction-following samples. This includes 178K captions, 960K open-ended QAs, and 196K multiple-choice QAs. 
            <d-figure id="fig-comparison" >
              <figure>
                  <img data-zoomable="" draggable="false" src="static/images/dataset-distribution.jpg" alt="benchmark category">
                  <figcaption>
                      <strong>Figure 4:</strong> Distribution of data across different datasets and question types (Caption, Open-ended, and Multi-Choice).
                  </figcaption>
              </figure>
            </d-figure>  
            
            <br>
            <br>
            <d-figure id="fig-comparison" >
              <figure>
                  <img data-zoomable="" draggable="false" src="static/images/llava_video_dataset_task_page-0001.jpg" alt="benchmark category">
                  <figcaption>
                      <strong>Figure 5:</strong> One example to illustrate the video instruction-following data.
                  </figcaption>
              </figure>
            </d-figure>  
            
            <h3 class="title is-4">Dataset Comparison</h3>
            <p>We provide a comparison of high-quality instruction-following video-language datasets, with a focus on synthetic data created with strong AI models, as shown in Table 1.</p>

            <ol>
                <li><strong>A broad collection of dynamic videos.</strong> In terms of video sources, although LLaVA-Hound contains the largest number of videos, 44% of its video data are sourced from <a href="https://ak.picdn.net/shutterstock/videos/21179416/preview/stock-footage-aerial-shot-winter-forest.mp4">WebVid</a>, where most videos are static. ShareGPT4Video includes 30% of its videos from <a href="https://www.pexels.com/video/a-bird-is-standing-on-the-beach-27916646/">Pexels</a>, 
                  ,<a href="https://pixabay.com/videos/plane-modelling-miniature-lockheed-134519/">Pixabay</a>, and <a href="https://mixkit.co/free-stock-video/a-young-woman-clad-in-snugly-black-sportswear-doing-lunges-52112/">Mixkit</a>, which are aesthetically good but also mostly static. Additionally, the majority of its videos come from Panda-70M, which are short clips from longer videos, suggesting simpler plots. In contrast, we carefully select video sources that offer dynamic, untrimmed videos with complex plots, which are crucial for developing a powerful video understanding model.
                </li>
                
                <li><strong>High frames per second.</strong> Regarding frame sampling in language annotations, the proposed dataset considers 1 FPS, while other datasets consider much lower FPS. LLaVA-Hound uniformly samples 10 frames from videos of any length. The average FPS is 0.008, which may miss some fine details. ShareGPT4Video picks key frames using CLIP based on frame uniqueness. This method might also miss subtle changes in the video because CLIP embeddings do not capture fine-grained dynamics well. Our method samples FPS=1 without using key frame selection algorithms, ensuring that detailed temporal information can be expressed in annotations with high coverage.</li>
                
                <li><strong>Diverse tasks.</strong> The proposed dataset considers three common task types, including caption, free-form, and closed-form QA, while existing datasets only consider a subset. Meanwhile, the quality and number of samples in our dataset is higher.</li>
            </ol>

            <div id="tab:dataset_comparison" style="display: flex; flex-direction: column; align-items: center;" class="figure">
              <div class="table-container" style="overflow-x:auto;">
                <table class="data-table" style="border-collapse: collapse; text-align: center;">
                  <thead>
                    <tr>
                      <th></th>
                      <th>Text</th>
                      <th>Video Source</th>
                      <th>#Video</th>
                      <th>Total Video Length</th>
                      <th>Average FPS</th>
                      <th>#Caption</th>
                      <th>#OE QA</th>
                      <th>#MC QA</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>LLaVA-Hound</td>
                      <td>GPT-4V</td>
                      <td><span>&#9733;</span></td>
                      <td>900K</td>
                      <td>3Khr</td>
                      <td>0.008</td>
                      <td>900K</td>
                      <td>900K</td>
                      <td>0</td>
                    </tr>
                    <tr>
                      <td>ShareGPT4Video</td>
                      <td>GPT-4V</td>
                      <td><span>&#9726;</span></td>
                      <td>40K</td>
                      <td>0.2Khr</td>
                      <td>0.15</td>
                      <td>40K</td>
                      <td>0</td>
                      <td>0</td>
                    </tr>
                    <tr class="highlight-orange">
                      <td>LLaVA-Video-178K</td>
                      <td>GPT-4o</td>
                      <td><span>&#x272A;</span></td>
                      <td>178K</td>
                      <td>2Khr</td>
                      <td>1</td>
                      <td>178K</td>
                      <td>960K</td>
                      <td>196K</td>
                    </tr>
                  </tbody>
                </table>
              </div>
              <figcaption>
                <strong>Table 1:</strong> Comparison of LLaVA-Video-178K and other video-language datasets. Average FPS represents the average number of frames per second that are used to prompt GPT-4o/GPT-4V for annotation. <span>&#9733;</span>: VIDAL, WebVid, ActivityNet. <span>&#9726;</span>: Panda-70M, Pexels, Pixabay, Mixkit, BDD100K, Ego4d. <span>&#x272A;</span>: HD-VILA-100M, Kinetics-700M, Ego4D, VidOR, InternVid, YouCook2, ActivityNet, Sth-sthv2, VIDAL, Charades.
              </figcaption>
            </div>          

        </div>


      </div>
    </div>
  </div>
</section>


<style>
  /* Table container for horizontal scrolling on smaller screens */
  .table-container {
    overflow-x: auto;
  }
  

  /* Responsive adjustments for mobile */
  @media only screen and (max-width: 768px) {
    .data-table th, .data-table td {
      padding: 6px 8px;
      font-size: 0.8em;
    }
    .data-table td span {
      font-size: 1.2em;
    }
  }
  
  </style>
  


<section class="section" id="Video Representation">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video Representation</h2>
        <div class="content has-text-justified">
          <p>
            Following the classic SlowFast idea in video representations, we develop 
            <span class="mathjax">\(\text{LLaVA-Video}_{~\mathtt{SlowFast}}\)</span> 
            to optimize the balance between the number of frames and the count of visual tokens, within the budget of the limited context window in LLM and GPU memory for video representation.
        </p>
        <p>
            Specifically, we categorize the frames into two groups, based on the strike rate <span class="mathjax">\(s\)</span>, where every <span class="mathjax">\(s\)</span> frames are uniformly selected to form the 
            <em>slow</em> frame group, and the rest of the frames are considered as the <em>fast</em> frame group. Note that a special case <span class="mathjax">\(s=1\)</span> leads to only one group, reducing the SlowFast representation to the original simple representation. 
            For each group, we apply different pooling rates using the PyTorch function <span class="mathjax">\(\mathtt{avg\_pool2d}()\)</span>. We apply 
            <span class="mathjax">\(p \times p\)</span> pooling and <span class="mathjax">\(2p \times 2p\)</span> pooling for slow and fast frames, respectively.
        </p>
        <p>
            To summarize, we parameterize the video representation configuration as 
            <span class="mathjax">\(\mathcal{V} = (T, M, s, p)\)</span>. 
        </p>


            <!-- <h3 class="title is-4">Video Sources</h3> -->
            <br><br>
            <d-figure id="fig-comparison" >
              <figure>
                  <img data-zoomable="" draggable="false" src="static/images/llava_video_arch_page-0001.jpg" alt="benchmark category">
                  <figcaption>
                      <strong>Figure 5:</strong> Video representations. A different number of tokens are utilized to represent frames.
                  </figcaption>
              </figure>
            </d-figure>
            

        </div>


      </div>
    </div>
  </div>
</section>




<section class="section" id="Benchmark Performance">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Benchmark Performance</h2>
        <div class="content has-text-justified">
          <p>
            We fine-tune LLaVA-OneVision (SI) on the joint dataset of video and image data. Specifically, we added video data from the LLaVA-Video-178K dataset and four public datasets: 
            ActivityNet-QA, NExT-QA, PerceptionTest, and LLaVA-Hound-255K, focusing on videos shorter than three minutes. These datasets were selected to improve our modelâ€™s performance, contributing to a total of 1.6 million video-language samples, which include 193,510 video descriptions, 1,240,801 open-ended questions, and 215,625 multiple-choice questions. Remarkably, 92.2% of the video descriptions, 77.4% of the open-ended questions, and 90.9% of the multiple-choice questions were newly annotated. Additionally, we used 1.1 million image-language pairs from the LLaVA-OneVision model.
          </p>

          <div id="tab:video_bench" style="display: flex; flex-direction: column; align-items: center;" class="figure">
            <div class="table-container" style="overflow-x:auto;"> <!-- Add scrollable container -->
              <table class="data-table" style="border-collapse: collapse; text-align: left;">
                <thead>
                  <tr style="border-bottom: 1px solid #000;"> <!-- Apply border to the row -->
                    <th></th> <!-- Empty cell with border -->
                    <th colspan="2">Caption</th>
                    <th colspan="2">Open-Ended Q&amp;A</th>
                    <th colspan="7" align="center">Multiple-Choice Q&amp;A</th>
                </tr>
                  <tr style="border-bottom: 1px solid #000;",align="center"> <!-- Border applied to the row -->
                    <th>Model</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">VideoDC</th>
                    <th class="rotate" style="border-right: 1px solid #000; border-bottom: 1px solid #000;">Dream-1K</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">ActNet-QA</th>
                    <th class="rotate" style="border-right: 1px solid #000; border-bottom: 1px solid #000;">VideoChatGPT</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">EgoSchema</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">MLVU</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">MVBench</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">NExT-QA</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">PerceptionTest</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">LongVideoBench</th>
                    <th class="rotate" style="border-bottom: 1px solid #000;">VideoMME</th>
                </tr>
                  <tr style="border-bottom: 2px solid #000;"> <!-- Added another border here -->
                      <th></th>
                      <th>test</th>
                      <th style="border-right: 1px solid #000;">test</th>
                      <th>test</th>
                      <th style="border-right: 1px solid #000;">test</th>
                      <th>test</th>
                      <th>m-avg</th>
                      <th>test</th>
                      <th>mc</th>
                      <th>val</th>
                      <th>val</th>
                      <th>wo/w-subs</th>
                  </tr>
              </thead>
                  <tbody>
                      <tr>
                          <td colspan="12" style="font-style: italic;">Proprietary models</td>
                      </tr>
                      <tr class="highlight-gray">
                          <td>GPT-4V</td>
                          <td>4.06</td>
                          <td style="border-right: 1px solid #000">34.4</td>
                          <td>57.0</td>
                          <td style="border-right: 1px solid #000">4.00</td>
                          <td>-</td>
                          <td>49.2</td>
                          <td>43.5</td>
                          <td>-</td>
                          <td>-</td>
                          <td>61.3</td>
                          <td>59.9/63.3</td>
                      </tr>
                      <tr class="highlight-gray">
                          <td>GPT-4o</td>
                          <td>-</td>
                          <td style="border-right: 1px solid #000">39.2</td>
                          <td>-</td>
                          <td style="border-right: 1px solid #000">-</td>
                          <td>-</td>
                          <td>64.6</td>
                          <td>-</td>
                          <td>-</td>
                          <td>-</td>
                          <td>66.7</td>
                          <td>71.9/77.2</td>
                      </tr>
                      <tr class="highlight-gray">
                          <td>Gemini-1.5-Flash</td>
                          <td>-</td>
                          <td style="border-right: 1px solid #000">34.8</td>
                          <td>55.3</td>
                          <td style="border-right: 1px solid #000">-</td>
                          <td>65.7</td>
                          <td>-</td>
                          <td>-</td>
                          <td>-</td>
                          <td>-</td>
                          <td>61.6</td>
                          <td>70.3/75.0</td>
                      </tr>
                      <tr class="highlight-gray">
                          <td>Gemini-1.5-Pro</td>
                          <td>-</td>
                          <td style="border-right: 1px solid #000">36.2</td>
                          <td>57.5</td>
                          <td style="border-right: 1px solid #000">-</td>
                          <td>72.2</td>
                          <td>-</td>
                          <td>-</td>
                          <td>-</td>
                          <td>-</td>
                          <td>64.0</td>
                          <td>75.0/81.3</td>
                      </tr>
                      <tr>
                          <td colspan="12" style="font-style: italic;">Open-source models</td>
                      </tr>
                      <tr>
                          <td>VILA-40B</td>
                          <td>3.37</td>
                          <td style="border-right: 1px solid #000">33.2</td>
                          <td>58.0</td>
                          <td style="border-right: 1px solid #000">3.36</td>
                          <td>58.0</td>
                          <td>-</td>
                          <td>-</td>
                          <td>67.9</td>
                          <td>54.0</td>
                          <td>-</td>
                          <td>60.1/61.1</td>
                      </tr>
                      <tr>
                          <td>PLLaVA-34B</td>
                          <td>-</td>
                          <td style="border-right: 1px solid #000">28.2</td>
                          <td>60.9</td>
                          <td style="border-right: 1px solid #000">3.48</td>
                          <td>-</td>
                          <td>-</td>
                          <td>58.1</td>
                          <td>-</td>
                          <td>-</td>
                          <td>53.2</td>
                          <td>-</td>
                      </tr>
                      <tr>
                          <td>LongVA-7B</td>
                          <td>3.14</td>
                          <td style="border-right: 1px solid #000">-</td>
                          <td>50.0</td>
                          <td style="border-right: 1px solid #000">3.20</td>
                          <td>-</td>
                          <td>56.3</td>
                          <td>-</td>
                          <td>68.3</td>
                          <td>-</td>
                          <td>-</td>
                          <td>52.6/54.3</td>
                      </tr>
                      <tr>
                          <td>IXC-2.5-7B</td>
                          <td>-</td>
                          <td style="border-right: 1px solid #000">-</td>
                          <td>52.8</td>
                          <td style="border-right: 1px solid #000">3.46</td>
                          <td>-</td>
                          <td>37.3</td>
                          <td>69.1</td>
                          <td>71.0</td>
                          <td>34.4</td>
                          <td>-</td>
                          <td>55.8/58.8</td>
                      </tr>
                      <tr>
                          <td>LLaVA-OV-7B</td>
                          <td>3.75</td>
                          <td style="border-right: 1px solid #000">31.7</td>
                          <td>56.6</td>
                          <td style="border-right: 1px solid #000">3.51</td>
                          <td>60.1</td>
                          <td>64.7</td>
                          <td>56.7</td>
                          <td>79.4*</td>
                          <td>57.1</td>
                          <td>56.5</td>
                          <td>58.2/61.5</td>
                      </tr>
                      <tr>
                        <td>VideoLLaMA2-72B</td>
                        <td>-</td>
                        <td style="border-right: 1px solid #000">27.1</td>
                        <td>55.2</td>
                        <td style="border-right: 1px solid #000">3.16</td>
                        <td>63.9</td>
                        <td>61.2</td>
                        <td>62.0</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>61.4/63.1</td>
                    </tr>
                      <tr>
                          <td>LLaVA-OV-72B</td>
                          <td>3.60</td>
                          <td style="border-right: 1px solid #000">33.2</td>
                          <td>62.3</td>
                          <td style="border-right: 1px solid #000">3.62</td>
                          <td>62.0</td>
                          <td>68.0</td>
                          <td>59.4</td>
                          <td>80.2*</td>
                          <td>66.9</td>
                          <td>61.3</td>
                          <td>66.2/69.5</td>
                      </tr>
                      <tr class="highlight-orange">
                          <td>LLaVA-Video-7B</td>
                          <td>3.66</td>
                          <td style="border-right: 1px solid #000">32.5</td>
                          <td>56.5*</td>
                          <td style="border-right: 1px solid #000">3.52</td>
                          <td>57.3</td>
                          <td>70.8</td>
                          <td>58.6</td>
                          <td>83.2*</td>
                          <td>67.9*</td>
                          <td>58.2</td>
                          <td>63.3/69.7</td>
                      </tr>
                      <tr class="highlight-orange">
                          <td>LLaVA-Video-72B</td>
                          <td>3.73</td>
                          <td style="border-right: 1px solid #000">34.0</td>
                          <td>63.4*</td>
                          <td style="border-right: 1px solid #000">3.62</td>
                          <td>65.6</td>
                          <td>74.4</td>
                          <td>64.1</td>
                          <td>85.4*</td>
                          <td>74.3*</td>
                          <td>61.9</td>
                          <td>70.5/76.9</td>
                      </tr>
                  </tbody>
              </table>
            </div>
            <figcaption>
                Table 2: LLaVA-Video performance on video benchmarks. We report the score out of 5 for VideoDC, VideoChatGPT while other results are reported in accuracy. All results are reported as 0-shot accuracy. *indicates that the training set has been observed in our data mixture.
            </figcaption>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Adjusted HTML for the Dream-1K Column -->
<!-- <th class="rotate" style="border-right: 1px solid #000; border-bottom: 1px solid #000;">Dream-1K</th> -->

<!-- Add this CSS -->
<style>
/* Ensure the table is responsive and properly formatted */
.table-container {
  overflow-x: auto;
}

/* General styling for the table */
.data-table {
  width: 100%;
  border-collapse: collapse;
}

.data-table th, .data-table td {
  padding: 8px 12px;
  border: 1px solid #ddd;
}

/* Styling for rotated table headers */
th.rotate {
  /* transform: rotate(-90deg); Rotate the text */
  white-space: nowrap; /* Ensure text doesn't wrap */
  padding: 0 10px; /* Adjust padding */
  height: 150px; /* Increase height to fit rotated text */
  vertical-align: bottom; /* Align text to bottom */
}

/* Add the border-right to the correct location */
th.rotate {
  border-right: 1px solid #000;
  border-bottom: 1px solid #000;
}

/* Responsive adjustments */
@media only screen and (max-width: 768px) {
  .title {
    font-size: 1.5em;
  }

  .content p {
    font-size: 0.9em;
  }

  .data-table th, .data-table td {
    padding: 6px 8px;
    font-size: 0.8em;
  }

  th.rotate {
    transform: rotate(-45deg);
    white-space: nowrap;
    padding: 0 10px;
    height: 60px;
    vertical-align: bottom;
  }
}
</style>






<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
            <p>
            This study introduces the LLaVA-Video-178K dataset, a high-quality synthetic dataset for video-language instruction-following. It is favored for its dense frame sampling rate in longer, untrimmed videos, covering diverse tasks such as captioning, open-ended and multi-choice QA. By training on the joint dataset of LLaVA-Video-178K with existing visual instruction tuning data, we developed a new model family, LLaVA-Video, which also considers video representation to effectively use GPU resources. This allows us to include more frames in the training process. The experimental results have demonstrated the effectiveness of the proposed synthetic dataset, and LLaVA-Video models have achieved excellent performance on a wide range of video benchmarks.
            </p>

        </div>


      </div>
    </div>
  </div>
</section>




<section class="section" id="Interactive Demos">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Interactive Demos</h2>
        <div class="content has-text-justified">
          <p>
            We provide interactive demos to showcase the capabilities of LLaVA-Video for realistic multimodal interactions.
          </p>
          <div id="results-carousel" class="carousel results-carousel">

            <!-- First Video Box -->
            <div class="box m-5">
              <div class="content has-text-centered">
                <div class="video-container">
                  <div id="player1"></div>
                </div>
                <p>LLaVA-Video teaches me how to download "TikTok" on my iPhone, step by step.</p>
              </div>
            </div>
            <!--/ First Video Box -->

            <!-- Second Video Box -->
            <div class="box m-5">
              <div class="content has-text-centered">
                <div class="video-container">
                  <div id="player2"></div>
                </div>
                <p>LLaVA-Video helps me find the healthy drink in the living room, and describe the living room.</p>
              </div>
            </div>
            <!--/ Second Video Box -->

          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract -->
  </div>
</section>

<script src="https://www.youtube.com/iframe_api"></script>
<script>
  var player1, player2;
  var players = []; // Array to keep track of all players

  function onYouTubeIframeAPIReady() {
    player1 = new YT.Player('player1', {
      videoId: 'csWFRIk7hv4',
      events: {
        'onStateChange': onPlayerStateChange
      }
    });
    players.push(player1); // Add player to array

    player2 = new YT.Player('player2', {
      videoId: 'A_jcAoqFndc',
      events: {
        'onStateChange': onPlayerStateChange
      }
    });
    players.push(player2); // Add player to array
  }

  // Function to handle video state changes
  function onPlayerStateChange(event) {
    if (event.data == YT.PlayerState.PLAYING) {
      // Stop all other players
      players.forEach(function(player) {
        if (player != event.target) {
          player.pauseVideo();
        }
      });
    }
  }
</script>

<style>
  /* Responsive iframe container with a consistent 16:9 aspect ratio */
  .video-container {
    position: relative;
    padding-bottom: 56.25%; /* This maintains the 16:9 aspect ratio */
    height: 0;
    overflow: hidden;
    max-width: 100%;
    background-color: #000;
  }
  
  .video-container iframe,
  .video-container div {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
  }
  
  /* Adjustments for mobile */
  @media only screen and (max-width: 768px) {
    .box {
      margin: 2vh 1vw;
    }
  }
  
  </style>
  








<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Related Blogs
        </h2>
        <div class="content has-text-justified">
          <ul>
            <li><a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a></li>
            <li><a href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/">LLaVA-NeXT: A Strong Zero-shot Video Understanding Model</a></li>
            <li><a href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/">LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild</a></li>
            <li><a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?</a></li>
            <li><a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models</a></li>
            <li><a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision: Easy Visual Task Transfer</a></li>
            <li><a href="https://llava-vl.github.io/blog/2024-10-03-llava-critic/">LLaVA-Critic: Learning to Evaluate Multimodal Models</a></li>
            <li><a href="https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/">Accelerating the Development of Large Multimodal Models with LMMs-Eval</a></li>
        </ul>

        </div>


      </div>
    </div>
  </div>
</section>

<!-- <div class="content has-text-justified">
  <ul>
      <li><a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a></li>
      <li><a href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/">LLaVA-NeXT: A Strong Zero-shot Video Understanding Model</a></li>
      <li><a href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/">LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild</a></li>
      <li><a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?</a></li>
      <li><a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models</a></li>
      <li><a href="https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/">Accelerating the Development of Large Multimodal Models with LMMs-Eval</a></li>
  </ul>
</div> -->

      

<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-top: 60px;">
    <h2 class="title is-3 has-text-centered">Citation</h2>
	<pre><code>@misc{zhang2024videoinstructiontuningsynthetic,
    title={Video Instruction Tuning With Synthetic Data}, 
    author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
    year={2024},
    eprint={2410.02713},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.02713}, 
}</code></pre>
  </div>
</section>

  	<!-- journal={arXiv preprint arXiv:2408.03326}, -->


</div>
<a class="u-url" href="/blog/2024-09-30-LLaVA-Video/" hidden></a>
</article>
</div>


<!-- <d-appendix>
  <h3>BibTeX</h3>
  <p class="bibtex">
      @article{tong2024cambrian,<br>
      &nbsp;&nbsp;title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},<br>
      &nbsp;&nbsp;author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng, and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining},<br>
      &nbsp;&nbsp;journal={arXiv preprint arXiv:2406.16860},<br>
      &nbsp;&nbsp;year={2024}<br>
      }
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>   

<d-bibliography src="main.bib"></d-bibliography>
<script src="./static/js/nav-bar.js"></script> -->


</body>
</html>
