<!DOCTYPE html>
<html>

    

<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title> LLaVA-OneVision: Easy Visual Task Transfer</title>

  <link rel="icon" type="image/x-icon" href="/blog/assets/images/logos/favicon.ico">


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>

  <style>
      .center-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100%;
            margin-top: -20px;
        }
    .node {
      fill: #f8f1e4;
      stroke: #000;
      stroke-width: 1;
      rx: 10;
      ry: 10;
    }
    .node text {
      font-size: 14px;
      text-anchor: middle;
    }
    .link {
      fill: none;
      stroke: #000;
      stroke-width: 2;
    }
    .badge {
      font-size: 12px;
    }
  </style>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold"  style="display: inline-block; margin-right: 200px;">
            <span style="vertical-align: middle">LLaVA-OneVision</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle" style="display: inline-block; margin-left: 210px;">
            Easy Visual Task Transfer
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://brianboli.com/">Bo Li</a><sup style="color:#ed4b82;">2,</sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            <span class="author-block">
              <a href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a><sup style="color:#ed4b82;">2,</sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/dongguoset">Dong Guo</a><sup style="color:#6fbf73;">1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zrrskywalker.github.io/">Renrui Zhang</a><sup style="color:#9400D3">3,</sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            
            <span class="author-block">
              <a href="https://fengli-ust.github.io/">Feng Li</a><sup style="color:#ffac33">4,</sup></sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            
            <span class="author-block">
              <a href="https://haozhang534.github.io/">Hao Zhang</a><sup style="color:#ffac33">4,</sup><sup style="color:#6fbf73;">â™¡</sup></span><br>
            
            <span class="author-block">
              <a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg">Kaichen Zhang</a><sup style="color:#ed4b82;">2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yanwei-li.com/">Yanwei Li</a><sup style="color:#9400D3">3,</sup><sup style="color:#6fbf73;">â™¡</sup>,</span>
            <span class="author-block">
              <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block" style="margin-bottom: 10px;">
              <a href="https://chunyuan.li/">Chunyuan Li</a><sup style="color:#6fbf73">1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#6fbf73;">1</sup>ByteDance,</span>
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#ed4b82">2</sup>NTU,</span>
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#9400D3">3</sup>CUHK,</span>
            <span class="author-block" style="margin-right: 15px;"><sup style="color:#ffac33">4</sup>HKUST</span><br>
            <span class="author-block"><sup style="color:#6fbf73;">â™¡</sup> Work collaborated with ByteDance</span><br>
            <!-- <span class="paper-block"><b style="color:#f41c1c">ICLR 2024 Oral</b> (85 in 7304, 1.2%)</span> -->
          </div>
        

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2408.03326"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2408.03326"
                   class="external-link button is-normal is-rounded is-dark">
                <!-- <a href="https://lupantech.github.io/papers/arxiv23_mathvista.pdf"
                   class="external-link button is-normal is-rounded is-dark"> -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/LLaVA-VL/LLaVA-NeXT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Training Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/lmms-lab"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>Checkpoints</span>
                </a>
              </span> 
              <!-- Visualization Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                      <!-- ðŸ”— -->
                  </span>
                  <span>LLaVA-OneVision Data</span>
                </a>
              </span> 
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="https://llava-onevision.lmms-lab.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">ðŸŽ¨</p>
                  </span>
                  <span>Live Demo</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
            <p>
				We present <b>LLaVA-OneVision</b>, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the <a href="https://llava-vl.github.io/blog/">LLaVA-NeXT blog series</a>. Our experimental results demonstrate show that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: <b>single-image, multi-image and video scenarios</b>. Importantly, the design of LLaVA-OneVision allow strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demosntrated through task transfer from images to videos.
            </p>

        </div>

    

      </div>
    </div>
  </div>

  <div class="columns is-centered"  style="margin-bottom: 90px;">
      <div class="content has-text-centered">
        <img src="demos/fig1.png" alt="data-overview" style="max-width: 55%;"/>
        <p> 
          <b>LLaVA-OneVision Network Architecture.</b> <br>
          Left: Current model instantiation; Right: The general form of LLaVA extended to more visual signals.
        </p> 
      </div>
    </div>

    <div class="columns is-centered">
      <div class="content has-text-centered">
        <img src="demos/fig3.png" alt="data-overview" style="max-width: 55%;"/>
        <p> 
          <b>Visual Representation Strategy in LLaVA-OneVision.</b><br>
The maximum number of visual tokens across different scenarios is designed to be similar,<br>
ensuring balanced representations to accommodate cross-scenario capability transfer.
        </p> 
      </div>
    </div>
    

      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>


<section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Open-source Release</h2>
          <div class="content has-text-justified">
            <ul>
                <p>
                We open-source the LLaVA-OneVision to facilitate future development of LMM in the community.
                </p>
                <li> <p style="font-size:18px"><i class="fab fa-github"></i>  <a href="https://github.com/LLaVA-VL/LLaVA-NeXT"> Training Code</a>: Reproduce our results with training and inference code</li>
                <li><p style="font-size:18px">ðŸ¤— <a href="https://huggingface.co/lmms-lab"> Checkpoints</a>: Access pre-trained model checkpoints (0.5B, 7B, 72B)</li>
                <li><p style="font-size:18px">ðŸ¤— <a href="https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data">LLaVA-OneVision Data</a>: Explore training datasets for Single-Image and OneVision stages</li>
                <li><p style="font-size:18px">ðŸŽ¨ <a href="https://llava-onevision.lmms-lab.com/">Live Demo</a>: Try it out yourself!</li>
            </ul>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Emerging Capabilities</h2>
        <div class="content has-text-justified">
        <p>
        In addition to reporting the LLaVA-OneVisionâ€™s capabilities across various benchmarks, we also observe the emerging behaviors of the proposed model with task transfer and composition, paving a promising way to generalize to tackle real-world computer vision tasks in the wild. We illustrate several emerging capabilities using examples as below.
      </p>
        <div id="results-carousel" class="carousel results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/df.png" alt="algebraic reasoning" width="100%"/>
              <p> LLaVA-OneVision transfers its ability to understand diagram and table to multi-image scenarios, interpreting multiple images in a coherent manner.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/gui.png" alt="arithmetic reasoning" width="80%"/>
              <p> LLaVA-OneVision plays the role of agent. It recognizes multiple screenshots on the iPhone and take action to interact with the iPhone, providing operation instructions for automating tasks.
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/som.png" alt="arithmetic reasoning" width="100%"/>
              <p> LLaVA-OneVision exhibits excellent set-of-mark prompting capabilities, ie, referring to marks when answering questions. This example demonstrates that describing specific objects based on numerical labels within an image highlights its comprehension skills in handling fine-grained visual content.
            </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/i2v.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                LLaVA-OneVision learns to generate detailed video creation prompts based on a static image. This capability is generalized to videos from the image-to-image language editing generation.
              </div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/video2.png" alt="arithmetic reasoning" width="100%"/>
              <p> 
                LLaVA-OneVision learns to analyze differences between videos with the same starting frame but different endings.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/video.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                LLaVA-OneVision learns to analyze differences between videos with similar backgrounds but different foreground objects.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/selfd.png" alt="arithmetic reasoning" width="50%"/>
              <p> 
                LLaVA-OneVision analyzes and interprets multi-camera video footage in self-driving contexts.</div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/subv.png" alt="arithmetic reasoning" width="60%"/>
              <p> 
                LLaVA-OneVision learns to understand and describe composed sub-videos in detail.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/vp.png" alt="arithmetic reasoning" width="50%"/>
              <p> 
                LLaVA-OneVision learns to provide detailed descriptions of highlighted subjects in video content.</div>
          </div>


          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="demos/messi.png" alt="arithmetic reasoning" width="50%"/>
              <p> 
                LLaVA-OneVisionâ€™s capability in referring image and video understanding. It accurately identifies the same individual in two images in the first instance. It identifies the same individual in both the image and the video in the second instance and correctly concludes the absence of the individual in the third instance, indicating its understanding capability to relate visual query in both image and video understanding.</div>
          </div>

      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>


<section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Development Roadmap of LLaVA-NeXT Series</h2>



          <div class="center-container" style="margin-top: -70px; margin-bottom: -90px; margin-left: 90px;">
            <svg width="900" height="400">
            
                <defs>
                    <marker id="arrowhead" markerWidth="5" markerHeight="3.5" 
                            refX="5" refY="1.75" orient="auto">
                        <polygon points="0 0, 5 1.75, 0 3.5" />
                    </marker>
                </defs>
                
                <!-- Nodes -->
                <rect x="50" y="150" width="200" height="50" fill="#fdecdc" stroke="black" stroke-width="1" rx="10" ry="10"></rect>
                <text x="78" y="180.5" font-family="Times New Roman" font-size="14">Jan</text>
                <circle cx="120" cy="176" r="5" fill="#b22222"></circle>
                <text x="140" y="180.5" font-family="Times New Roman" font-size="14" style="font-weight:bold;"><a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/" fill="blue">LLaVA-NeXT</a></text>
            
                <rect x="300" y="50" width="200" height="50" fill="#f0f8ff" stroke="black" stroke-width="1" rx="10" ry="10"></rect>
                <text x="328" y="79" font-family="Times New Roman" font-size="14">April</text>
                <circle cx="375" cy="74.5" r="5" fill="#1e90ff"></circle>
                <text x="390" y="70" font-family="Times New Roman" font-size="14">LLaVA-NeXT</text>
                <text x="390" y="90" font-family="Times New Roman" font-size="14" style="font-weight:bold;"><a href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/" fill="blue">(Video)</a></text>
            
                <rect x="300" y="125" width="200" height="50" fill="#f0f8ff" stroke="black" stroke-width="1" rx="10" ry="10"></rect>
                <text x="328" y="154" font-family="Times New Roman" font-size="14">May</text>
                <circle cx="375" cy="149.5" r="5" fill="#1e90ff"></circle>
                <text x="390" y="145" font-family="Times New Roman" font-size="14">LLaVA-NeXT</text>
                <text x="390" y="165" font-family="Times New Roman" font-size="14" style="font-weight:bold;"><a href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/" fill="blue">(Stronger)</a></text>
            
                <rect x="300" y="200" width="200" height="50" fill="#f0f8ff" stroke="black" stroke-width="1" rx="10" ry="10"></rect>
                <text x="328" y="229" font-family="Times New Roman" font-size="14">May</text>
                <circle cx="375" cy="224.5" r="5" fill="#1e90ff"></circle>
                <text x="390" y="220" font-family="Times New Roman" font-size="14">LLaVA-NeXT</text>
                <text x="390" y="240" font-family="Times New Roman" font-size="14" style="font-weight:bold;"><a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/" fill="blue">(Ablations)</a></text>
            
                <rect x="300" y="275" width="200" height="50" fill="#f0f8ff" stroke="black" stroke-width="1" rx="10" ry="10"></rect>
                <text x="328" y="304" font-family="Times New Roman" font-size="14">June</text>
                <circle cx="375" cy="299.5" r="5" fill="#1e90ff"></circle>
                <text x="390" y="295" font-family="Times New Roman" font-size="14">LLaVA-NeXT</text>
                <text x="390" y="315" font-family="Times New Roman" font-size="14" style="font-weight:bold;"><a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/" fill="blue">(Interleave)</a></text>
            
                <rect x="550" y="150" width="240" height="50" fill="#e6ffe6" stroke="black" stroke-width="1" rx="10" ry="10"></rect>
                <text x="578" y="180.5" font-family="Times New Roman" font-size="14">July</text>
                <circle cx="625" cy="176" r="5" fill="#008000"></circle>
                <text x="640" y="180.5" font-family="Times New Roman" font-size="14" style="font-weight:bold;">LLaVA-OneVision</text>
            
                <!-- Links -->
                <line x1="250" y1="175" x2="300" y2="75" stroke="black" stroke-width="2" marker-end="url(#arrowhead)"></line>
                <line x1="250" y1="175" x2="300" y2="150" stroke="black" stroke-width="2"  marker-end="url(#arrowhead)"></line>
                <line x1="250" y1="175" x2="300" y2="225" stroke="black" stroke-width="2"  marker-end="url(#arrowhead)"></line>
                <line x1="250" y1="175" x2="300" y2="300" stroke="black" stroke-width="2"  marker-end="url(#arrowhead)"></line>
                <line x1="500" y1="75" x2="550" y2="175" stroke="black" stroke-width="2"  marker-end="url(#arrowhead)"></line>
                <line x1="500" y1="150" x2="550" y2="175" stroke="black" stroke-width="2"  marker-end="url(#arrowhead)"></line>
                <line x1="500" y1="225" x2="550" y2="175" stroke="black" stroke-width="2"  marker-end="url(#arrowhead)"></line>
                <line x1="500" y1="300" x2="550" y2="175" stroke="black" stroke-width="2"  marker-end="url(#arrowhead)"></line>
            </svg>
            </div>



          <div class="content has-text-justified">
            <ul>
                <li><a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a></li>
                <li><a href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/">LLaVA-NeXT: A Strong Zero-shot Video Understanding Model</a></li>
                <li><a href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/">LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild</a></li>
                <li><a href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?</a></li>
                <li><a href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models</a></li>
                <li><a href="https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/">Accelerating the Development of Large Multimodal Models with LMMs-Eval</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
  </div>
  </section>

                    

<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-top: 60px;">
    <h2 class="title is-3 has-text-centered">Citation</h2>
    <pre><code>@misc{li2024llava-onevision,
    author = {Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
    title = {LLaVA-OneVision: Easy Visual Task Transfer},
    url = {https://llava-vl.github.io/blog/2024-08-05-llava-onevision/},
    month = {August}
    year = {2024}
}</code></pre>
  </div>
</section>



</div>
<a class="u-url" href="/blog/2024-08-05-llava-onevision/" hidden></a>
</article>
</div>


</body>
</html>
